{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  <a href=\"https://colab.research.google.com/github/marcpalo1999/MIA_sanidad/blob/main/2_2_Machine_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Aprendizaje Supervisado para Clasificación Médica: Predicción de Enfermedad Coronaria\n",
    "\n",
    "\n",
    "\n",
    " ## Objetivos\n",
    "\n",
    " En este script construiremos modelos de Machine Learning para clasificación médica usando el dataset de enfermedades cardíacas (nivel de estenosis coronaria) para predecir qué pacientes necesitan cateterismo cardíaco y cuales se lo pueden ahorrar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "if not os.path.exists('/content/MIA_sanidad'):\n",
    "    !git clone https://github.com/marcpalo1999/MIA_sanidad.git\n",
    "os.chdir('/content/MIA_sanidad')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importar librerías necesarias\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import (confusion_matrix, accuracy_score, precision_score, \n",
    "                             recall_score, f1_score, classification_report, roc_curve, auc)\n",
    "\n",
    "# Modelos de clasificación\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 0: Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar el dataset\n",
    "df = pd.read_csv(\"./data/heart_disease_dataset_con_nulos.csv\")\n",
    "\n",
    "print(f\"Dataset cargado: {df.shape[0]} pacientes, {df.shape[1]} variables\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # SECCIÓN 0.1: Resumen Descripción del Problema Clínico\n",
    "\n",
    " Para un entendimiento completo del problema, leed el archivo markdown adjunto titulado `2_2_ML_Explicación_Datos.md`.\n",
    "\n",
    "\n",
    "\n",
    " **Contexto del problema:**\n",
    "\n",
    "\n",
    "\n",
    " La secuencia diagnóstica actual para enfermedad coronaria es:\n",
    "\n",
    " 1. Evaluación clínica (edad, síntomas, factores de riesgo)\n",
    "\n",
    " 2. ECG en reposo\n",
    "\n",
    " 3. Prueba de esfuerzo con ECG\n",
    "\n",
    "      - Si es ambigua → Gammagrafía cardíaca (mínimamente invasiva), para descartar\n",
    "         \n",
    "         - si la gammagrafia es positiva vamos a cateterismo terapeutico\n",
    "         - si es gammagrafia es negativa dejamos en seguimiento/profilaxis \n",
    "\n",
    "      - Si positiva → Cateterismo, primeramente diagnostico (INVASIVO, riesgos, coste alto ~3000€)\n",
    "\n",
    "\n",
    "\n",
    " **Problema clínico:**\n",
    "\n",
    " - Muchos pacientes se someten a cateterismo innecesariamente\n",
    "\n",
    " - El cateterismo tiene riesgos (2% complicaciones: hematomas, nefropatía, etc.)\n",
    "\n",
    " - Alto coste económico\n",
    "\n",
    "\n",
    "\n",
    " **Solución propuesta con ML:**\n",
    "\n",
    " Predecir el resultado del cateterismo usando solo las pruebas no invasivas previas,\n",
    "\n",
    " para reducir cateterismos innecesarios manteniendo alta sensibilidad (no perder casos reales de enfermedad).\n",
    "\n",
    "\n",
    "\n",
    " **Dos estrategias de modelado:**\n",
    "\n",
    "\n",
    "\n",
    " 1. **Modelo Básico**: Solo consulta + ECG + prueba esfuerzo (aplicable a TODOS los pacientes)\n",
    "\n",
    "    - Variables: edad, sexo, tipo dolor torácico, presión arterial, colesterol, ECG reposo, resultados prueba esfuerzo\n",
    "\n",
    "    - Objetivo: Screening inicial para evitar cateterismos innecesarios\n",
    "\n",
    "\n",
    "\n",
    " 2. **Modelo Completo**: Incluye gammagrafía (solo para casos equívocos)\n",
    "\n",
    "    - Variables: todas las anteriores + resultado gammagrafía (thal)\n",
    "\n",
    "    - Objetivo: Optimizar decisión cuando ya se realizó gammagrafía\n",
    "\n",
    "\n",
    "\n",
    " **Variable excluida por data leakage:**\n",
    "\n",
    " - `ca` (número de vasos obstruidos visualizados en fluoroscopia) se obtiene DURANTE el cateterismo\n",
    "\n",
    " - Usar `ca` para predecir `num` (resultado cateterismo) es circular, ya que ambas se miden simultáneamente\n",
    "\n",
    " - Incluirla sería como hacer trampa: predecir el resultado usando información del mismo procedimiento\n",
    "\n",
    "\n",
    "\n",
    " **Binarización del target:**\n",
    "\n",
    " - Variable original `num`: 0 (sin obstrucción coronatia), 1-4 (diferentes grados de obstruccion en diferentes cavidades)\n",
    "\n",
    " - Simplificamos a binario: 0 = no necesita cateterismo, 1 = necesita cateterismo\n",
    "\n",
    " - Razón clínica: la decisión es binaria (hacer/no hacer cateterismo), no gradual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 1: Entendiendo el Problema desde ML\n",
    "\n",
    "\n",
    "\n",
    " **Tipo de problema:**\n",
    "\n",
    " - Supervised Learning: tenemos variable objetivo (target) para entrenar\n",
    "\n",
    " - Classification: la variable objetivo es categórica (enfermedad: sí/no)\n",
    "\n",
    "\n",
    "\n",
    " **Objetivo:** Dado un nuevo paciente con sus características clínicas, predecir si tiene\n",
    "\n",
    " enfermedad coronaria significativa que requiera cateterismo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analizar variable objetivo 'num'\n",
    "print(\"Distribución original de 'num':\")\n",
    "print(df['num'].value_counts().sort_index())\n",
    "\n",
    "# Convertir a problema binario, ademas ya codificamos 0 como no enfermedad y 1 como enfermedad, no con strings, ya que si fuesen strings tendriamos que pasarlo a 0/1\n",
    "df['target'] = (df['num'] > 0).astype(int)\n",
    "\n",
    "print(\"\\nDistribución binaria (target):\")\n",
    "print(df['target'].value_counts())\n",
    "print(f\"\\nPrevalencia de enfermedad: {df['target'].mean()*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Observación:** El dataset está relativamente balanceado (~45% con enfermedad).\n",
    "\n",
    " Esto es favorable para el aprendizaje del modelo, aunque no refleja la prevalencia\n",
    "\n",
    " real en screening poblacional (sería mucho menor). Estos pacientes ya fueron\n",
    "\n",
    " seleccionados para cateterismo basándose en criterio clínico previo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 2: Análisis Exploratorio Rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Información general del dataset\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revisar valores nulos\n",
    "missing_counts = df.isna().sum().sort_values(ascending=False)\n",
    "print(\"Número de missing values por columna:\")\n",
    "print(missing_counts)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estadísticas descriptivas\n",
    "df.describe()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Observaciones:**\n",
    "\n",
    " - Variables con diferentes escalas (edad: 29-77, colesterol: 126-564)\n",
    "\n",
    " - Algunas variables son categóricas codificadas como números\n",
    "\n",
    " - Presencia de valores faltantes que requieren manejo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Verificación de duplicados - Evitar data leakage\n",
    "\n",
    "\n",
    "\n",
    " **Problema:** Si el mismo paciente aparece múltiples veces en el dataset, podría\n",
    "\n",
    " terminar en AMBOS conjuntos (train y test), causando data leakage severo.\n",
    "\n",
    "\n",
    "\n",
    " **Consecuencia:** El modelo \"vería\" al paciente durante el entrenamiento y luego\n",
    "\n",
    " lo \"evaluaría\" en el test, inflando artificialmente las métricas.\n",
    "\n",
    "\n",
    "\n",
    " **Solución:** Verificar que cada fila representa un paciente único."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar duplicados completos (filas idénticas)\n",
    "duplicados_completos = df.duplicated().sum()\n",
    "\n",
    "if duplicados_completos > 0:\n",
    "    print(f\"ALERTA: {duplicados_completos} filas duplicadas encontradas\")\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"Duplicados eliminados. Dataset: {df.shape[0]} pacientes\")\n",
    "else:\n",
    "    print(f\"OK: No hay duplicados ({df.shape[0]} pacientes únicos)\")\n",
    "\n",
    "# Verificar duplicados parciales basados en variables clave\n",
    "columnas_clave = ['age', 'sex', 'trestbps', 'chol', 'thalach']\n",
    "duplicados_parciales = df.duplicated(subset=columnas_clave).sum()\n",
    "\n",
    "if duplicados_parciales > 0:\n",
    "    print(f\"ALERTA: {duplicados_parciales} posibles pacientes repetidos (verificar manualmente)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 3: Preprocesamiento - Parte 1: Codificación de Variables Categóricas\n",
    "\n",
    "\n",
    "\n",
    " Las variables categóricas necesitan One-Hot Encoding para ser interpretadas correctamente por parte de los modelos de ML.\n",
    "\n",
    " Sin él, el modelo pensaría que cp=4 es \"mayor\" que cp=2, lo cual puede tener sentido clinico en algunos casos (variables categoricas ordinales), pero no en todos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos ver, los autores del dataset muy amablemente ya nos han codificado las variables categoricas ['cp', 'restecg', 'slope'] como numéricas. Entonces no tenemos que hacer el mapeo str -> int. Lo que una variable categorica de Ints puede ser un problema para un modelo de ML si realmente no es ordinal, ya que decidle al modelo que la variable tiene valores del 1 al 4, implicitamente le indica que la categoria 4 > 3 > 2 > 1. Para evitar este efecto indeseado (si fuese indeseado) se usa el one-hot encoding, que aplicaremos ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables categóricas en el dataset\n",
    "categorical_cols = ['cp', 'restecg', 'slope']\n",
    "categorical_cols = [col for col in categorical_cols if col in df.columns]\n",
    "\n",
    "print(f\"Variables categóricas identificadas: {categorical_cols}\")\n",
    "for col in categorical_cols:\n",
    "    print(f\"  {col}: {df[col].nunique()} categorías - valores: {sorted(df[col].unique())}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aplicar One-Hot Encoding\n",
    "df_encoded = pd.get_dummies(df, columns=categorical_cols, drop_first=True, dtype=int)\n",
    "\n",
    "print(f\"\\nShape antes: {df.shape}\")\n",
    "print(f\"Shape después: {df_encoded.shape}\")\n",
    "print(f\"Columnas añadidas: {df_encoded.shape[1] - df.shape[1]}\")\n",
    "\n",
    "df_encoded.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 4: Definición de Estrategias de Modelado\n",
    "\n",
    "\n",
    "\n",
    " Implementaremos DOS estrategias según disponibilidad de datos clínicos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variables disponibles (excluir target, num original, y ca por data leakage)\n",
    "all_features = [col for col in df_encoded.columns if col not in ['target', 'num', 'ca']]\n",
    "\n",
    "# ESTRATEGIA 1: Modelo Básico (sin gammagrafía thal, sin cateterismo ca)\n",
    "basic_features = [col for col in all_features if not col.startswith('thal')]\n",
    "\n",
    "# ESTRATEGIA 2: Modelo Completo (con gammagrafía thal, sin cateterismo ca)\n",
    "complete_features = all_features.copy()\n",
    "\n",
    "print(f\"ESTRATEGIA 1 - Modelo Básico:\")\n",
    "print(f\"  Total variables: {len(basic_features)}\")\n",
    "print(f\"  Variables: {basic_features}\\n\")\n",
    "\n",
    "print(f\"ESTRATEGIA 2 - Modelo Completo:\")\n",
    "print(f\"  Total variables: {len(complete_features)}\")\n",
    "print(f\"  Variables añadidas: {[col for col in complete_features if col not in basic_features]}\\n\")\n",
    "\n",
    "print(f\"Variable EXCLUIDA (data leakage): 'ca'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 5: División Train/Test\n",
    "\n",
    "\n",
    "\n",
    " ## El problema fundamental del ML\n",
    "\n",
    "\n",
    "\n",
    " Si entrenamos y evaluamos en los mismos datos, el modelo memorizará en lugar de aprender.\n",
    "\n",
    " Como darle a un estudiante las preguntas del examen antes del examen.\n",
    "\n",
    "\n",
    "\n",
    " ## Solución: Train/Test Split\n",
    "\n",
    "\n",
    "\n",
    " - **Train set (80%):** Entrenar el modelo\n",
    "\n",
    " - **Test set (20%):** Completamente oculto, simula pacientes nuevos\n",
    "\n",
    "\n",
    "\n",
    " El test set se usa UNA SOLA VEZ al final para estimar performance real."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos\n",
    "X = df_encoded[complete_features]\n",
    "y = df_encoded['target']\n",
    "\n",
    "# Verificar tipos numéricos\n",
    "non_numeric = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "if non_numeric:\n",
    "    print(f\"Eliminando columnas no numéricas: {non_numeric}\")\n",
    "    X = X.select_dtypes(include=[np.number])\n",
    "\n",
    "# Train/Test Split (80/20) con estratificación para mantener proporción de clases\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Dimensiones X: {X.shape}\")\n",
    "print(f\"Train: {X_train.shape[0]} pacientes ({X_train.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"Test: {X_test.shape[0]} pacientes ({X_test.shape[0]/len(X)*100:.1f}%)\")\n",
    "print(f\"\\nVerificación de estratificación:\")\n",
    "print(f\"  Enfermedad en Train: {y_train.mean()*100:.1f}%\")\n",
    "print(f\"  Enfermedad en Test: {y_test.mean()*100:.1f}%\")\n",
    "print(f\"  Enfermedad en Original: {y.mean()*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 6: Preprocesamiento - Parte 2: Valores Faltantes (DESPUÉS del split para evitar data leakage)\n",
    "\n",
    "\n",
    "\n",
    " ## Por qué DESPUÉS del split\n",
    "\n",
    "\n",
    "\n",
    " Los modelos ML no pueden trabajar con valores NaN. Usaremos mediana para imputación (robusta a outliers).\n",
    "\n",
    "\n",
    "\n",
    " **Flujo incorrecto:**\n",
    "\n",
    " 1. Calcular mediana con todos los datos (train + test)\n",
    "\n",
    " 2. Imputar valores faltantes\n",
    "\n",
    " 3. Hacer train/test split\n",
    "\n",
    "\n",
    "\n",
    " **Problema:** La mediana fue calculada usando información del test set, causando data leakage.\n",
    "\n",
    "\n",
    "\n",
    " **Flujo correcto:**\n",
    "\n",
    " 1. Hacer train/test split primero\n",
    "\n",
    " 2. Calcular mediana SOLO con datos de train\n",
    "\n",
    " 3. Aplicar esa mediana a train\n",
    "\n",
    " 4. Aplicar la MISMA mediana (del train) al test\n",
    "\n",
    "\n",
    "\n",
    " **Analogía clínica:** Es como calcular valores de referencia de un test diagnóstico.\n",
    "\n",
    " Los valores de referencia se calculan con la población de desarrollo, NO incluyendo\n",
    "\n",
    " la población de validación. Luego se aplican esos mismos valores a la validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_with_nulls = X_train.columns[X_train.isnull().any()].tolist()\n",
    "\n",
    "if len(columns_with_nulls) > 0:\n",
    "    print(f\"Variables con valores nulos: {columns_with_nulls}\")\n",
    "    print(f\"\\nValores nulos antes de imputación:\")\n",
    "    print(f\"  Train: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"  Test: {X_test.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Imputación con mediana calculada SOLO del train\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    imputer.fit(X_train)  # Aprende SOLO de train\n",
    "    \n",
    "    X_train = pd.DataFrame(\n",
    "        imputer.transform(X_train),\n",
    "        columns=X_train.columns,\n",
    "        index=X_train.index\n",
    "    )\n",
    "    \n",
    "    X_test = pd.DataFrame(\n",
    "        imputer.transform(X_test),  # Usa medianas del train\n",
    "        columns=X_test.columns,\n",
    "        index=X_test.index\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nValores nulos después de imputación:\")\n",
    "    print(f\"  Train: {X_train.isnull().sum().sum()}\")\n",
    "    print(f\"  Test: {X_test.isnull().sum().sum()}\")\n",
    "else:\n",
    "    print(\"No hay valores nulos en train ni test\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 7: Escalado (DESPUÉS del split para evitar data leakage)\n",
    "\n",
    "\n",
    "\n",
    " ## Por qué escalar\n",
    "\n",
    " Las variables tienen escalas muy diferentes (edad: 29-77, colesterol: 126-564).\n",
    "\n",
    " Algunos algoritmos (Logistic Regression, KNN) son sensibles a estas diferencias, porque estan basados internamente en metricas de \"distancia\".\n",
    "\n",
    "\n",
    "\n",
    " ## StandardScaler\n",
    "\n",
    " Transforma a media=0, desviación estándar=1.\n",
    "\n",
    "\n",
    "\n",
    " ## Por qué DESPUÉS del split\n",
    "\n",
    " Si escalamos antes, calcularíamos estadísticas usando también el test set, lo cual sería data leakage.\n",
    "\n",
    "\n",
    "\n",
    " **Flujo correcto:**\n",
    "\n",
    " 1. Split train/test\n",
    "\n",
    " 2. Calcular media/std SOLO del train\n",
    "\n",
    " 3. Aplicar transformación a ambos sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit en train (aprende media y std)\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "# Transform en test (usa estadísticas del train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Convertir a DataFrames\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "X_train_scaled.describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Efectivamente nuestras variables continuas ahora tienen media 0 y std 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 8: Modelo Baseline\n",
    "\n",
    "\n",
    "\n",
    " ## Por qué necesitamos un baseline\n",
    "\n",
    "\n",
    "\n",
    " Antes de construir modelos complejos, necesitamos un punto de referencia:\n",
    "\n",
    " el enfoque más simple posible.\n",
    "\n",
    "\n",
    "\n",
    " **DummyClassifier** siempre predice la clase más frecuente.\n",
    "\n",
    " Es nuestro \"mínimo a superar\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_clf = DummyClassifier(strategy='most_frequent', random_state=42)\n",
    "dummy_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "dummy_train_pred = dummy_clf.predict(X_train_scaled)\n",
    "dummy_test_pred = dummy_clf.predict(X_test_scaled)\n",
    "\n",
    "dummy_train_acc = accuracy_score(y_train, dummy_train_pred)\n",
    "dummy_test_acc = accuracy_score(y_test, dummy_test_pred)\n",
    "\n",
    "most_frequent_class = dummy_clf.classes_[np.argmax(dummy_clf.class_prior_)]\n",
    "\n",
    "print(f\"BASELINE MODEL (DummyClassifier)\")\n",
    "print(f\"  Train Accuracy: {dummy_train_acc*100:.2f}%\")\n",
    "print(f\"  Test Accuracy: {dummy_test_acc*100:.2f}%\")\n",
    "print(f\"  Predicción: siempre {'Enfermedad' if most_frequent_class == 1 else 'Sin Enfermedad'}\")\n",
    "print(f\"\\nCualquier modelo ML debe superar {dummy_test_acc*100:.1f}% para ser útil\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 9: Entendiendo la Evaluación de Performance\n",
    "\n",
    "\n",
    "\n",
    " ## Tres formas de evaluar un modelo:\n",
    "\n",
    " En el fondo siempre que evaluamos un modelo la pregunta que nos hacemos es: Como de bien lo hará nuestro modelo en \"produccion\", es decir, cuando lo estemos usando en el departamento que le toque? Como no lo sabemos (porque aun no lo hemos puesto en producción), lo que intentamos es estimar lo mas precisamente posible como lo hará, y tres maneras de estimar este error en producción (aunque unas mejores que otras) son las siguientes:\n",
    "\n",
    " **A) Train Accuracy:** Detectar underfitting (modelo demasiado simple)\n",
    "\n",
    "\n",
    "\n",
    " **B) Test Accuracy:** Performance real (usar SOLO al final, UNA VEZ)\n",
    "\n",
    "\n",
    "\n",
    " **C) Cross-Validation:** Método preferido durante desarrollo del modelo\n",
    "\n",
    "\n",
    "\n",
    " Durante desarrollo usamos SOLO los datos de train para no \"contaminar\" el test set.\n",
    "\n",
    " El mismo ejercicio que estamos haciendo para la metrica de accuracy lo podriamos hacer para cualquier otra metrica de performance, somo sensitividad, especificidad etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demostración con Logistic Regression\n",
    "demo_model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "demo_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "# A) Train Accuracy\n",
    "train_pred = demo_model.predict(X_train_scaled)\n",
    "train_acc = accuracy_score(y_train, train_pred)\n",
    "\n",
    "# B) Test Accuracy\n",
    "test_pred = demo_model.predict(X_test_scaled)\n",
    "test_acc = accuracy_score(y_test, test_pred)\n",
    "\n",
    "# C) Cross-Validation\n",
    "cv_scores = cross_val_score(demo_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"TRES FORMAS DE EVALUAR UN MODELO:\\n\")\n",
    "print(f\"A) TRAIN ACCURACY: {train_acc*100:.2f}%\")\n",
    "print(\"   El modelo 've' estos datos durante entrenamiento\")\n",
    "print(\"   Score alto NO garantiza funcionar en pacientes nuevos\\n\")\n",
    "\n",
    "print(f\"B) TEST ACCURACY: {test_acc*100:.2f}%\")\n",
    "print(\"   El modelo NUNCA vio estos datos\")\n",
    "print(\"   Estima performance real\")\n",
    "print(\"   Solo hacer esto UNA VEZ al final\\n\")\n",
    "\n",
    "print(f\"C) CROSS-VALIDATION (5-Fold):\")\n",
    "print(f\"   Scores por fold: {[f'{s*100:.1f}%' for s in cv_scores]}\")\n",
    "print(f\"   Media: {cv_scores.mean()*100:.2f}% (± {cv_scores.std()*100:.2f}%)\")\n",
    "print(\"   Usa solo training data (divide en 5 folds los datos de train)\")\n",
    "print(\"   Da múltiples estimaciones (más robusto)\")\n",
    "print(\"   Test set permanece intacto\")\n",
    "print(\"\\nDurante desarrollo: USAR CROSS-VALIDATION\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Regla de oro:**\n",
    "\n",
    " - Si Train >> CV: OVERFITTING\n",
    "\n",
    " - Si Train ≈ CV ≈ Test: GOOD GENERALIZATION\n",
    "\n",
    " - Durante desarrollo: Cross-Validation\n",
    "\n",
    " - Solo al final: Test (una vez)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 10: ESTRATEGIA 1 - Modelo Básico (Sin Gammagrafía)\n",
    "\n",
    "\n",
    "\n",
    " Entrenamos modelos usando solo variables de consulta + prueba esfuerzo.\n",
    "\n",
    " Aplicable a TODOS los pacientes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparar datos para Modelo Básico\n",
    "X_train_basic = X_train_scaled[basic_features]\n",
    "X_test_basic = X_test_scaled[basic_features]\n",
    "\n",
    "print(f\"ESTRATEGIA 1: MODELO BÁSICO\")\n",
    "print(f\"Variables usadas: {len(basic_features)} de {len(complete_features)} totales\")\n",
    "print(f\"Excluidas: thal (gammagrafía), ca (cateterismo)\")\n",
    "print(f\"Train: {X_train_basic.shape}, Test: {X_test_basic.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1: Logistic Regression (Básico)\n",
    "log_reg_basic = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg_basic.fit(X_train_basic, y_train)\n",
    "\n",
    "log_reg_basic_train = log_reg_basic.score(X_train_basic, y_train)\n",
    "log_reg_basic_cv = cross_val_score(log_reg_basic, X_train_basic, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 1: LOGISTIC REGRESSION (Básico)\")\n",
    "print(f\"Train: {log_reg_basic_train*100:.2f}% | CV: {log_reg_basic_cv.mean()*100:.2f}% (±{log_reg_basic_cv.std()*100:.2f}%) | Gap: {(log_reg_basic_train - log_reg_basic_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2: Decision Tree (Básico)\n",
    "tree_basic = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "tree_basic.fit(X_train_basic, y_train)\n",
    "\n",
    "tree_basic_train = tree_basic.score(X_train_basic, y_train)\n",
    "tree_basic_cv = cross_val_score(tree_basic, X_train_basic, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 2: DECISION TREE (Básico)\")\n",
    "print(f\"Train: {tree_basic_train*100:.2f}% | CV: {tree_basic_cv.mean()*100:.2f}% (±{tree_basic_cv.std()*100:.2f}%) | Gap: {(tree_basic_train - tree_basic_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 3: Random Forest (Básico)\n",
    "rf_basic = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_basic.fit(X_train_basic, y_train)\n",
    "\n",
    "rf_basic_train = rf_basic.score(X_train_basic, y_train)\n",
    "rf_basic_cv = cross_val_score(rf_basic, X_train_basic, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 3: RANDOM FOREST (Básico)\")\n",
    "print(f\"Train: {rf_basic_train*100:.2f}% | CV: {rf_basic_cv.mean()*100:.2f}% (±{rf_basic_cv.std()*100:.2f}%) | Gap: {(rf_basic_train - rf_basic_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 4: KNN (Básico)\n",
    "knn_basic = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_basic.fit(X_train_basic, y_train)\n",
    "\n",
    "knn_basic_train = knn_basic.score(X_train_basic, y_train)\n",
    "knn_basic_cv = cross_val_score(knn_basic, X_train_basic, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 4: K-NEAREST NEIGHBORS (Básico)\")\n",
    "print(f\"Train: {knn_basic_train*100:.2f}% | CV: {knn_basic_cv.mean()*100:.2f}% (±{knn_basic_cv.std()*100:.2f}%) | Gap: {(knn_basic_train - knn_basic_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 11: ESTRATEGIA 2 - Modelo Completo (Con Gammagrafía)\n",
    "\n",
    "\n",
    "\n",
    " Entrenamos modelos incluyendo resultado de gammagrafía.\n",
    "\n",
    " Solo aplicable a pacientes que ya tienen gammagrafía."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ESTRATEGIA 2: MODELO COMPLETO\")\n",
    "print(f\"Variables usadas: {len(complete_features)}\")\n",
    "print(f\"Incluye: thal (gammagrafía)\")\n",
    "print(f\"Excluye: ca (cateterismo - data leakage)\")\n",
    "print(f\"Train: {X_train_scaled.shape}, Test: {X_test_scaled.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 1: Logistic Regression (Completo)\n",
    "log_reg_complete = LogisticRegression(random_state=42, max_iter=1000)\n",
    "log_reg_complete.fit(X_train_scaled, y_train)\n",
    "\n",
    "log_reg_complete_train = log_reg_complete.score(X_train_scaled, y_train)\n",
    "log_reg_complete_cv = cross_val_score(log_reg_complete, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 1: LOGISTIC REGRESSION (Completo)\")\n",
    "print(f\"Train: {log_reg_complete_train*100:.2f}% | CV: {log_reg_complete_cv.mean()*100:.2f}% (±{log_reg_complete_cv.std()*100:.2f}%) | Gap: {(log_reg_complete_train - log_reg_complete_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 2: Decision Tree (Completo)\n",
    "tree_complete = DecisionTreeClassifier(random_state=42, max_depth=5)\n",
    "tree_complete.fit(X_train_scaled, y_train)\n",
    "\n",
    "tree_complete_train = tree_complete.score(X_train_scaled, y_train)\n",
    "tree_complete_cv = cross_val_score(tree_complete, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 2: DECISION TREE (Completo)\")\n",
    "print(f\"Train: {tree_complete_train*100:.2f}% | CV: {tree_complete_cv.mean()*100:.2f}% (±{tree_complete_cv.std()*100:.2f}%) | Gap: {(tree_complete_train - tree_complete_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 3: Random Forest (Completo)\n",
    "rf_complete = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
    "rf_complete.fit(X_train_scaled, y_train)\n",
    "\n",
    "rf_complete_train = rf_complete.score(X_train_scaled, y_train)\n",
    "rf_complete_cv = cross_val_score(rf_complete, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 3: RANDOM FOREST (Completo)\")\n",
    "print(f\"Train: {rf_complete_train*100:.2f}% | CV: {rf_complete_cv.mean()*100:.2f}% (±{rf_complete_cv.std()*100:.2f}%) | Gap: {(rf_complete_train - rf_complete_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modelo 4: KNN (Completo)\n",
    "knn_complete = KNeighborsClassifier(n_neighbors=5)\n",
    "knn_complete.fit(X_train_scaled, y_train)\n",
    "\n",
    "knn_complete_train = knn_complete.score(X_train_scaled, y_train)\n",
    "knn_complete_cv = cross_val_score(knn_complete, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "print(\"MODELO 4: K-NEAREST NEIGHBORS (Completo)\")\n",
    "print(f\"Train: {knn_complete_train*100:.2f}% | CV: {knn_complete_cv.mean()*100:.2f}% (±{knn_complete_cv.std()*100:.2f}%) | Gap: {(knn_complete_train - knn_complete_cv.mean())*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 12: Comparación de Estrategias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tabla comparativa\n",
    "comparison = pd.DataFrame({\n",
    "    'Modelo': ['Baseline', \n",
    "               'LogReg (Básico)', 'Tree (Básico)', 'RF (Básico)', 'KNN (Básico)',\n",
    "               'LogReg (Completo)', 'Tree (Completo)', 'RF (Completo)', 'KNN (Completo)'],\n",
    "    'Estrategia': ['N/A',\n",
    "                   'Básico', 'Básico', 'Básico', 'Básico',\n",
    "                   'Completo', 'Completo', 'Completo', 'Completo'],\n",
    "    'Train (%)': [dummy_train_acc*100,\n",
    "                  log_reg_basic_train*100, tree_basic_train*100, rf_basic_train*100, knn_basic_train*100,\n",
    "                  log_reg_complete_train*100, tree_complete_train*100, rf_complete_train*100, knn_complete_train*100],\n",
    "    'CV (%)': [dummy_test_acc*100,\n",
    "               log_reg_basic_cv.mean()*100, tree_basic_cv.mean()*100, rf_basic_cv.mean()*100, knn_basic_cv.mean()*100,\n",
    "               log_reg_complete_cv.mean()*100, tree_complete_cv.mean()*100, rf_complete_cv.mean()*100, knn_complete_cv.mean()*100],\n",
    "    'Gap (%)': [0,\n",
    "                (log_reg_basic_train - log_reg_basic_cv.mean())*100,\n",
    "                (tree_basic_train - tree_basic_cv.mean())*100,\n",
    "                (rf_basic_train - rf_basic_cv.mean())*100,\n",
    "                (knn_basic_train - knn_basic_cv.mean())*100,\n",
    "                (log_reg_complete_train - log_reg_complete_cv.mean())*100,\n",
    "                (tree_complete_train - tree_complete_cv.mean())*100,\n",
    "                (rf_complete_train - rf_complete_cv.mean())*100,\n",
    "                (knn_complete_train - knn_complete_cv.mean())*100]\n",
    "})\n",
    "\n",
    "print(\"COMPARACIÓN COMPLETA: BÁSICO vs COMPLETO\")\n",
    "print(comparison.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización comparativa\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "basic_models = comparison[comparison['Estrategia'] == 'Básico']\n",
    "complete_models = comparison[comparison['Estrategia'] == 'Completo']\n",
    "\n",
    "x = np.arange(4)\n",
    "width = 0.35\n",
    "\n",
    "color_basic = '#5DADE2'\n",
    "color_complete = '#F39C12'\n",
    "\n",
    "axes[0].bar(x - width/2, basic_models['CV (%)'].values, width, \n",
    "            label='Básico (sin gammagrafía)', alpha=0.85, color=color_basic, edgecolor='#2874A6', linewidth=1.5)\n",
    "axes[0].bar(x + width/2, complete_models['CV (%)'].values, width,\n",
    "            label='Completo (con gammagrafía)', alpha=0.85, color=color_complete, edgecolor='#BA4A00', linewidth=1.5)\n",
    "axes[0].set_ylabel('CV Accuracy (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Comparación: Modelo Básico vs Completo', fontweight='bold', fontsize=13)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(['LogReg', 'Tree', 'RF', 'KNN'])\n",
    "axes[0].axhline(y=dummy_test_acc*100, color='#E74C3C', linestyle='--', alpha=0.6, linewidth=2, \n",
    "                label=f'Baseline ({dummy_test_acc*100:.1f}%)')\n",
    "axes[0].legend(framealpha=0.9, loc='lower right')\n",
    "axes[0].set_ylim([40, 100])\n",
    "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "all_models = comparison[comparison['Estrategia'] != 'N/A']\n",
    "colors_gap = ['#27AE60' if gap < 10 else '#F39C12' if gap < 20 else '#E74C3C' for gap in all_models['Gap (%)']]\n",
    "\n",
    "axes[1].scatter(range(len(all_models)), all_models['Gap (%)'], \n",
    "                c=colors_gap, s=200, alpha=0.7, edgecolors='black', linewidth=1.5)\n",
    "\n",
    "axes[1].axhline(y=10, color='#F39C12', linestyle='--', alpha=0.5, linewidth=2, label='Moderado (10%)')\n",
    "axes[1].axhline(y=20, color='#E74C3C', linestyle='--', alpha=0.5, linewidth=2, label='Alto (20%)')\n",
    "\n",
    "axes[1].set_ylabel('Gap (Train - CV) %', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Gap como Indicador de Overfitting', fontweight='bold', fontsize=13)\n",
    "axes[1].set_xticks(range(len(all_models)))\n",
    "axes[1].set_xticklabels(all_models['Modelo'], rotation=45, ha='right')\n",
    "axes[1].legend(framealpha=0.9)\n",
    "axes[1].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "axes[1].spines['top'].set_visible(False)\n",
    "axes[1].spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Observaciones:**\n",
    "\n",
    " - Todos los modelos superan el baseline\n",
    "\n",
    " - Modelos Completos (con gammagrafía) tienen mejor accuracy: Esperable, puesto que usan también una información valiosa que el modelo basico no usa...\n",
    "\n",
    " - Random Forest muestra mejor balance en ambas estrategias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 13: Métricas Clínicas - Evaluación en Test Set\n",
    "\n",
    "\n",
    "\n",
    " ## Métricas que importan en medicina:\n",
    "\n",
    " Como me habreis oido decir antes, la accuracy no lo es todo, puesto que si el coste monetario o humano de un FP y un FN no son parecidos, no nos ayuda a encontrar un buen compromiso, de la misma manera que también nos engaña si predecimos sobre enfermedades con muy poca prevalencia (nuestros datos estan muy desbalanceados)\n",
    "\n",
    " **Sensitivity (Recall):** De todos los enfermos, ¿cuántos detectamos?\n",
    "\n",
    " - En medicina: NO queremos perder casos de enfermedad (priorizar Sensitivity)\n",
    "\n",
    "\n",
    "\n",
    " **Specificity:** De todos los sanos, ¿cuántos identificamos correctamente?\n",
    "\n",
    " - Evita procedimientos innecesarios\n",
    "\n",
    "\n",
    "\n",
    " **PPV (Precision):** Si predecimos \"enfermedad\", ¿qué probabilidad de estar en lo cierto?\n",
    "\n",
    "\n",
    "\n",
    " **NPV:** Si predecimos \"sano\", ¿qué probabilidad de estar en lo cierto?\n",
    "\n",
    "\n",
    "\n",
    " Evaluamos SOLO EN TEST SET (una única vez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clinical_metrics(y_true, y_pred, model_name):\n",
    "    \"\"\"Calcula métricas clínicas relevantes\"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    \n",
    "    print(f\"\\nMÉTRICAS CLÍNICAS: {model_name}\")\n",
    "    print(f\"{'='*70}\")\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(f\"  TN: {TN} (sanos identificados correctamente)\")\n",
    "    print(f\"  FP: {FP} (sanos clasificados como enfermos - cateterismo innecesario)\")\n",
    "    print(f\"  FN: {FN} (enfermos clasificados como sanos - PELIGROSO)\")\n",
    "    print(f\"  TP: {TP} (enfermos identificados correctamente)\")\n",
    "    \n",
    "    print(f\"\\nMÉTRICAS:\")\n",
    "    print(f\"  Accuracy: {accuracy*100:.1f}%\")\n",
    "    print(f\"  Sensitivity (Recall): {sensitivity*100:.1f}% - Detecta {sensitivity*100:.0f}% de enfermos\")\n",
    "    print(f\"  Specificity: {specificity*100:.1f}% - Identifica {specificity*100:.0f}% de sanos\")\n",
    "    print(f\"  PPV (Precision): {ppv*100:.1f}% - Si predice enfermedad, {ppv*100:.0f}% correcto\")\n",
    "    print(f\"  NPV: {npv*100:.1f}% - Si predice sano, {npv*100:.0f}% correcto\")\n",
    "    \n",
    "    print(f\"\\nCASOS PERDIDOS: {FN} pacientes con enfermedad NO detectados\")\n",
    "    \n",
    "    return {\n",
    "        'Model': model_name,\n",
    "        'Sensitivity': sensitivity * 100,\n",
    "        'Specificity': specificity * 100,\n",
    "        'PPV': ppv * 100,\n",
    "        'NPV': npv * 100,\n",
    "        'Accuracy': accuracy * 100,\n",
    "        'FN': FN,\n",
    "        'FP': FP\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar Random Forest Básico en Test\n",
    "print(\"EVALUACIÓN FINAL EN TEST SET\")\n",
    "\n",
    "y_pred_rf_basic = rf_basic.predict(X_test_basic)\n",
    "metrics_rf_basic = clinical_metrics(y_test, y_pred_rf_basic, \"Random Forest (Básico)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluar Random Forest Completo en Test\n",
    "y_pred_rf_complete = rf_complete.predict(X_test_scaled)\n",
    "metrics_rf_complete = clinical_metrics(y_test, y_pred_rf_complete, \"Random Forest (Completo)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparación visual de métricas\n",
    "metrics_comparison = pd.DataFrame([metrics_rf_basic, metrics_rf_complete])\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "metrics_to_plot = ['Sensitivity', 'Specificity', 'PPV', 'NPV']\n",
    "x = np.arange(len(metrics_to_plot))\n",
    "width = 0.35\n",
    "\n",
    "basic_vals = [metrics_rf_basic[m] for m in metrics_to_plot]\n",
    "complete_vals = [metrics_rf_complete[m] for m in metrics_to_plot]\n",
    "\n",
    "color_basic = '#5DADE2'\n",
    "color_complete = '#F39C12'\n",
    "\n",
    "axes[0].bar(x - width/2, basic_vals, width, label='Básico', alpha=0.85, \n",
    "            color=color_basic, edgecolor='#2874A6', linewidth=1.5)\n",
    "axes[0].bar(x + width/2, complete_vals, width, label='Completo', alpha=0.85, \n",
    "            color=color_complete, edgecolor='#BA4A00', linewidth=1.5)\n",
    "axes[0].set_ylabel('Porcentaje (%)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Métricas Clínicas: Básico vs Completo (Random Forest)', fontweight='bold', fontsize=13)\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels(metrics_to_plot)\n",
    "axes[0].legend(framealpha=0.9)\n",
    "axes[0].set_ylim([0, 100])\n",
    "axes[0].axhline(y=80, color='#27AE60', linestyle='--', alpha=0.4, linewidth=2)\n",
    "axes[0].grid(axis='y', alpha=0.3, linestyle='--')\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "error_types = ['Falsos Negativos\\n(Enfermos perdidos)', 'Falsos Positivos\\n(Cateterismos innecesarios)']\n",
    "basic_errors = [metrics_rf_basic['FN'], metrics_rf_basic['FP']]\n",
    "complete_errors = [metrics_rf_complete['FN'], metrics_rf_complete['FP']]\n",
    "\n",
    "x2 = np.arange(len(error_types))\n",
    "\n",
    "axes[1].hlines(y=x2 - 0.15, xmin=0, xmax=basic_errors, color=color_basic, alpha=0.7, linewidth=3)\n",
    "axes[1].plot(basic_errors, x2 - 0.15, \"o\", markersize=12, color=color_basic, \n",
    "             alpha=0.85, label='Básico', markeredgecolor='#2874A6', markeredgewidth=1.5)\n",
    "\n",
    "axes[1].hlines(y=x2 + 0.15, xmin=0, xmax=complete_errors, color=color_complete, alpha=0.7, linewidth=3)\n",
    "axes[1].plot(complete_errors, x2 + 0.15, \"o\", markersize=12, color=color_complete, \n",
    "             alpha=0.85, label='Completo', markeredgecolor='#BA4A00', markeredgewidth=1.5)\n",
    "\n",
    "axes[1].set_xlabel('Número de casos', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Tipos de Error: Básico vs Completo', fontweight='bold', fontsize=13)\n",
    "axes[1].set_yticks(x2)\n",
    "axes[1].set_yticklabels(error_types)\n",
    "axes[1].legend(framealpha=0.9)\n",
    "axes[1].grid(axis='x', alpha=0.3, linestyle='--')\n",
    "axes[1].spines['top'].set_visible(False)\n",
    "axes[1].spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # SECCIÓN 14: Curvas ROC y Análisis de Threshold\n",
    "\n",
    "\n",
    "\n",
    " ## Por qué importa el threshold\n",
    "\n",
    "\n",
    "\n",
    " Los modelos de clasificación devuelven una probabilidad de enfermedad (0 a 1).\n",
    "\n",
    " Necesitamos un umbral de decisión (threshold) para convertir esa probabilidad en una decisión binaria. Hasta ahora automatiamente el codigo habia usado un thrshold de 0.5 para binarizar nuestra prediction (que es una probabilidad entre 0 y 1) a un resultado (sano/enfermo)\n",
    "\n",
    "\n",
    "\n",
    " **Threshold por defecto: 0.5**\n",
    "\n",
    " - Si probabilidad >= 0.5, predecir \"enfermedad\"\n",
    "\n",
    " - Si probabilidad < 0.5, predecir \"sano\"\n",
    "\n",
    "\n",
    "\n",
    " **Problema:** Este 0.5 es arbitrario y puede no ser óptimo para decisiones clínicas.\n",
    "\n",
    "\n",
    "\n",
    " **En medicina:**\n",
    "\n",
    " - **Threshold bajo (ej: 0.3):** Más sensible, detecta más enfermos, pero más falsos positivos\n",
    "\n",
    " - **Threshold alto (ej: 0.7):** Más específico, menos falsos positivos, pero pierde casos reales\n",
    "\n",
    "\n",
    "\n",
    " La **curva ROC** muestra el trade-off entre sensibilidad y especificidad para TODOS los thresholds posibles. \n",
    " \n",
    " **Importante: Deberemos identificar el threshold que es más util para nuestras necesidades clínicas**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alerta: El modelo ya lo habiamos entrenado antes y escogido en train, cogiendo el que tenia mejor accuracy en cross-validation en TRAIN. Ahora todo lo estamos comprobando en TEST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtener probabilidades predichas\n",
    "y_proba_rf_basic = rf_basic.predict_proba(X_test_basic)[:, 1]\n",
    "fpr_basic, tpr_basic, thresholds_basic = roc_curve(y_test, y_proba_rf_basic)\n",
    "auc_basic = auc(fpr_basic, tpr_basic)\n",
    "\n",
    "y_proba_rf_complete = rf_complete.predict_proba(X_test_scaled)[:, 1]\n",
    "fpr_complete, tpr_complete, thresholds_complete = roc_curve(y_test, y_proba_rf_complete)\n",
    "auc_complete = auc(fpr_complete, tpr_complete)\n",
    "\n",
    "print(f\"AUC Modelo Básico: {auc_basic:.3f}\")\n",
    "print(f\"AUC Modelo Completo: {auc_complete:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ROC-AUC es una metrica de performance threshold acnostic, es decir, a diferencia de la accuracy, senstivity etc., AUC no depende de que threshold escojamos, sinó que nos da una intuicion de lo bien que lo hace el modelo en general."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualización: Curvas ROC\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 7))\n",
    "\n",
    "color_basic = '#5DADE2'\n",
    "color_complete = '#F39C12'\n",
    "\n",
    "axes[0].plot(fpr_basic, tpr_basic, linewidth=3, color=color_basic, alpha=0.85,\n",
    "             label=f'Básico (AUC = {auc_basic:.3f})')\n",
    "axes[0].plot(fpr_complete, tpr_complete, linewidth=3, color=color_complete, alpha=0.85,\n",
    "             label=f'Completo (AUC = {auc_complete:.3f})')\n",
    "\n",
    "axes[0].plot([0, 1], [0, 1], 'k--', linewidth=2, alpha=0.4, label='Aleatorio (AUC = 0.5)')\n",
    "\n",
    "idx_basic_05 = np.argmin(np.abs(thresholds_basic - 0.5))\n",
    "axes[0].scatter(fpr_basic[idx_basic_05], tpr_basic[idx_basic_05], \n",
    "                s=150, c=color_basic, edgecolors='black', linewidth=2, zorder=5,\n",
    "                marker='o', label=f'Threshold=0.5 (Básico)')\n",
    "\n",
    "idx_complete_05 = np.argmin(np.abs(thresholds_complete - 0.5))\n",
    "axes[0].scatter(fpr_complete[idx_complete_05], tpr_complete[idx_complete_05], \n",
    "                s=150, c=color_complete, edgecolors='black', linewidth=2, zorder=5,\n",
    "                marker='s', label=f'Threshold=0.5 (Completo)')\n",
    "\n",
    "axes[0].set_xlabel('1 - Especificidad (Tasa Falsos Positivos)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_ylabel('Sensibilidad (Tasa Verdaderos Positivos)', fontsize=12, fontweight='bold')\n",
    "axes[0].set_title('Curvas ROC: Básico vs Completo', fontsize=14, fontweight='bold')\n",
    "axes[0].legend(loc='lower right', framealpha=0.95, fontsize=10)\n",
    "axes[0].grid(alpha=0.3, linestyle='--')\n",
    "axes[0].set_xlim([-0.02, 1.02])\n",
    "axes[0].set_ylim([-0.02, 1.02])\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "specificity_complete = 1 - fpr_complete\n",
    "\n",
    "youden_idx = np.argmax(tpr_complete - fpr_complete)\n",
    "optimal_threshold = thresholds_complete[youden_idx]\n",
    "\n",
    "axes[1].plot(thresholds_complete, tpr_complete, linewidth=3, color='#27AE60', \n",
    "             alpha=0.85, label='Sensibilidad')\n",
    "axes[1].plot(thresholds_complete, specificity_complete, linewidth=3, color='#E74C3C', \n",
    "             alpha=0.85, label='Especificidad')\n",
    "\n",
    "axes[1].axvline(optimal_threshold, color='gray', linestyle='--', linewidth=2, alpha=0.6)\n",
    "axes[1].scatter(optimal_threshold, tpr_complete[youden_idx], s=150, c='#27AE60', \n",
    "                edgecolors='black', linewidth=2, zorder=5)\n",
    "axes[1].scatter(optimal_threshold, specificity_complete[youden_idx], s=150, c='#E74C3C', \n",
    "                edgecolors='black', linewidth=2, zorder=5)\n",
    "axes[1].text(optimal_threshold + 0.05, 0.5, f'Óptimo\\n{optimal_threshold:.2f}', \n",
    "             fontsize=11, fontweight='bold', ha='left')\n",
    "\n",
    "axes[1].axvline(0.5, color='black', linestyle=':', linewidth=2, alpha=0.5, label='Default (0.5)')\n",
    "\n",
    "axes[1].set_xlabel('Threshold de decisión', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Tasa (%)', fontsize=12, fontweight='bold')\n",
    "axes[1].set_title('Trade-off Sensibilidad vs Especificidad\\n(Modelo Completo)', \n",
    "                  fontsize=14, fontweight='bold')\n",
    "axes[1].legend(loc='best', framealpha=0.95, fontsize=10)\n",
    "axes[1].grid(alpha=0.3, linestyle='--')\n",
    "axes[1].set_xlim([0, 1])\n",
    "axes[1].set_ylim([0, 1.05])\n",
    "axes[1].spines['top'].set_visible(False)\n",
    "axes[1].spines['right'].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nThreshold óptimo (Youden's Index): {optimal_threshold:.3f}\")\n",
    "print(f\"Sensibilidad en threshold óptimo: {tpr_complete[youden_idx]*100:.1f}%\")\n",
    "print(f\"Especificidad en threshold óptimo: {specificity_complete[youden_idx]*100:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Interpretación de las curvas ROC:**\n",
    "\n",
    "\n",
    "\n",
    " - **AUC (Area Under Curve):** Métrica global de performance\n",
    "\n",
    "   - AUC = 0.5: Modelo aleatorio (línea diagonal)\n",
    "\n",
    "   - AUC = 1.0: Modelo perfecto (esquina superior izquierda)\n",
    "\n",
    "   - AUC > 0.8: Generalmente considerado bueno en medicina\n",
    "\n",
    "\n",
    "\n",
    " - **Threshold óptimo (Youden's Index):** Maximiza (Sensibilidad + Especificidad - 1)\n",
    "\n",
    "   - Balancea ambas métricas igualmente\n",
    "\n",
    "   - Puede no ser óptimo si hay costes asimétricos (FN vs FP)\n",
    "\n",
    "\n",
    "\n",
    " - **En nuestro contexto clínico:**\n",
    "\n",
    "   - FN (enfermo no detectado) es MÁS grave que FP (cateterismo innecesario)\n",
    "\n",
    "   - Podríamos preferir threshold < 0.5 para aumentar sensibilidad\n",
    "\n",
    "   - Trade-off: Más cateterismos pero menos casos perdidos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Análisis de diferentes thresholds\n",
    "thresholds_to_test = np.linspace(0,1, num=51)\n",
    "results = []\n",
    "\n",
    "for threshold in thresholds_to_test:\n",
    "    y_pred_threshold = (y_proba_rf_complete >= threshold).astype(int)\n",
    "    \n",
    "    cm = confusion_matrix(y_test, y_pred_threshold)\n",
    "    TN, FP, FN, TP = cm.ravel()\n",
    "    \n",
    "    sensitivity = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    specificity = TN / (TN + FP) if (TN + FP) > 0 else 0\n",
    "    ppv = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    npv = TN / (TN + FN) if (TN + FN) > 0 else 0\n",
    "    \n",
    "    results.append({\n",
    "        'Threshold': threshold,\n",
    "        'Sensitivity': sensitivity * 100,\n",
    "        'Specificity': specificity * 100,\n",
    "        'Accuracy': (TP + TN) / (TP + TN + FP + FN) * 100,\n",
    "        'PPV': ppv * 100,\n",
    "        'NPV': npv * 100,\n",
    "        'FN': FN,\n",
    "        'FP': FP,\n",
    "        'TN': TN,\n",
    "        'Total_Errors': FN + FP\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nAnálisis de diferentes thresholds (Modelo Completo):\")\n",
    "print(results_df.round(3).to_string(index=False))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Interpretación clínica de los thresholds:**\n",
    "\n",
    " Gracias a esta visualización, ahora podremos escoger el threshold que nos de unas metricas más utiles para nuestras necesidades.\n",
    "\n",
    "\n",
    " **Decisión clínica:**\n",
    "\n",
    " En nuestro caso, que no nos podemos permitir perder casos de enfermedad,\n",
    "\n",
    " un threshold alrededor de 0.26 podría ser el más apropiado, puesto que no estariamos pasando por alto ningun positivo, y estamos descartando un ~64% de pacientes sanos que ya no tendrán que hacerse el cateterismo.\n",
    "\n",
    " Evidentemente todo esto se tendria que testear en un entorno clínico real para ver si los resultados se mantienen. (En un dataset de validación externa)\n",
    "\n",
    " ## Victoria!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECCIÓN 15: Importancia de las variables (Si hay tiempo)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECCIÓN 16: Entrenamiento final.\n",
    "\n",
    "Ahora que ya tenemos el mejor modelo y el mejor threshold, podemos entrenarlo con todos los datos, guardarlo y ponerlo en producción!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " # FIN DEL SCRIPT\n",
    "\n",
    "\n",
    "\n",
    " ## Conceptos Cubiertos:\n",
    "\n",
    " - Preparación de datos completa (missing values, encoding, scaling)\n",
    "\n",
    " - Identificación y manejo de data leakage (variable `ca`, pacientes repetidos)\n",
    "\n",
    " - **CRÍTICO: Imputación y escalado DESPUÉS del split para evitar data leakage**\n",
    "\n",
    " - Dos estrategias de modelado (Básico vs Completo)\n",
    "\n",
    " - Train/Test Split y Cross-Validation\n",
    "\n",
    " - Modelos baseline y 4 algoritmos de clasificación\n",
    "\n",
    " - Evaluación correcta (Train/Test/CV)\n",
    "\n",
    " - Métricas clínicas (Sensitivity, Specificity, PPV, NPV)\n",
    "\n",
    " - Curvas ROC y análisis de threshold\n",
    "\n",
    " - Impacto clínico real (ahorro económico, reducción de complicaciones)\n",
    "\n",
    " - Trade-offs clínicos (casos detectados vs procedimientos evitados)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "marc_priv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
